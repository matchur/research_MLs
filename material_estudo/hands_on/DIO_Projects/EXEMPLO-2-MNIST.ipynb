{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d687f646-7460-4afc-821c-f3854839cd23",
   "metadata": {},
   "source": [
    "# EXEMPLO 2 - Rede Neural. Estudo de Caso: Mnist.\n",
    "\n",
    "Neste exemplo, vamos utilizar uma Rede Neural para resolver um problema clássico de classificação: o reconhecimento de dígitos escritos à mão, utilizando o dataset MNIST.\n",
    "\n",
    "O MNIST é um conjunto de dados muito conhecido na área de Machine Learning.\n",
    "Ele contém milhares de imagens em preto e branco de dígitos de 0 a 9, escritas à mão,\n",
    "e o objetivo é treinar um modelo que consiga identificar corretamente qual número aparece em cada imagem.\n",
    "\n",
    "Vamos usar uma Rede Neural simples (usando PyTorch) para aprender os padrões dessas imagens\n",
    "e, depois do treinamento, testar sua capacidade de reconhecer novos dígitos.\n",
    "\n",
    "Esse exemplo é muito útil para entender como redes neurais funcionam em problemas visuais\n",
    "e serve como porta de entrada para tarefas mais complexas de Visão Computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65aebd8b-fa3e-43a4-8148-492b9ae1f6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32868b68-5150-4df1-a95b-8ba6759477b0",
   "metadata": {},
   "source": [
    "## Carregando e preparando o dataset MNIST.\n",
    "\n",
    "Primeiro, usamos *transforms.ToTensor()* para converter as imagens para tensores do PyTorch\n",
    "e normalizar os valores dos pixels (de 0–255 para 0–1).\n",
    "\n",
    "Em seguida, carregamos o dataset MNIST com datasets.MNIST(), indicando o caminho onde os dados ficarão salvos.\n",
    "O parâmetro download=True baixa os dados se ainda não existirem no diretório.\n",
    "**Atenção**: train=False indica que estamos pegando o conjunto de TESTE — use train=True se quiser o conjunto de treino.\n",
    "\n",
    "Depois, usamos DataLoader para organizar os dados em batches (lotes) de 64 imagens e embaralhá-los a cada época.\n",
    "Isso é importante para acelerar e estabilizar o treinamento da rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8387065-ea4c-41a7-b927-58cac486ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor();\n",
    "\n",
    "trainset = datasets.MNIST('C:\\\\Users\\\\Matheus\\\\Documents\\\\Github Projects\\\\research_MLs\\\\material_estudo\\\\hands_on\\\\datasets\\\\mnist',download=True,train=True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)\n",
    "\n",
    "valset = datasets.MNIST('C:\\\\Users\\\\Matheus\\\\Documents\\\\Github Projects\\\\research_MLs\\\\material_estudo\\\\hands_on\\\\datasets\\\\mnist',download=True,train=False,transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(valset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff5ab0-01af-4628-bac1-7ceacf4de37f",
   "metadata": {},
   "source": [
    "Vamos visualizar uma imagem real do dataset MNIST.\n",
    "\n",
    "Primeiro, usamos iter(trainloader) para criar um iterador sobre o DataLoader,\n",
    "e usamos next() para pegar o primeiro lote de imagens e etiquetas.\n",
    "\n",
    "Depois, selecionamos a primeira imagem do lote (imagens[0]),\n",
    "convertemos para NumPy com .numpy() e removemos dimensões extras com .squeeze().\n",
    "\n",
    "Por fim, mostramos a imagem usando plt.imshow() com a coloração em tons de cinza.\n",
    "Isso nos permite visualizar os dígitos reais que o modelo vai aprender a reconhecer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dffa9ab6-a9c0-4930-8a8a-bb0480c53b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "imagens, etiquetas = next(dataiter)\n",
    "\n",
    "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')\n",
    "plt.title(f\"Etiqueta: {etiquetas[0].item()}\")\n",
    "plt.axis('off')  # opcional: tira os eixos\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de23568-221a-4408-8c4d-67e031393c14",
   "metadata": {},
   "source": [
    "Vamos inspecionar as dimensões (shape) da imagem e da etiqueta.\n",
    "\n",
    "A imagem (imagens[0]) tem shape [1, 28, 28], onde:\n",
    "- 1 representa o canal da imagem (escala de cinza),\n",
    "- 28x28 é a resolução da imagem em pixels.\n",
    "\n",
    "Já a etiqueta (etiquetas[0]) é um valor escalar, como 0, 1, 2, ..., 9,\n",
    "que representa qual número está escrito na imagem. Por isso, ela não tem shape — é só um número.\n",
    "\n",
    "### Por que isso é importante?\n",
    "Verificar o shape dos dados nos ajuda a entender como a rede neural irá recebê-los.\n",
    "Se a dimensão dos dados estiver errada, isso pode causar erros no treinamento,\n",
    "como incompatibilidades entre camadas ou falhas na visualização.\n",
    "\n",
    "Essa etapa também ajuda a desenvolver a intuição sobre como os dados estão organizados internamente\n",
    "e é essencial para evitar bugs difíceis de detectar mais à frente no projeto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5edca099-fa32-41ad-b1ad-3a47adf46b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(imagens[0].shape)\n",
    "print(etiquetas[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f6045-0e97-46fa-beca-03cb5a3f2ad2",
   "metadata": {},
   "source": [
    "### COMPARAÇÃO ENTRE DUAS ARQUITETURAS DE REDES NEURAIS\n",
    "\n",
    "CODE 1 - Arquitetura com 3 camadas lineares (mais profunda)\n",
    "Neste modelo, passamos a imagem por duas camadas ocultas com ReLU,\n",
    "depois aplicamos uma camada final com log_softmax para gerar probabilidades normalizadas.\n",
    "Essa saída é ideal para ser usada com a função de perda NLLLoss.\n",
    "\n",
    "CODE 2 - Arquitetura com 2 camadas lineares (mais simples)\n",
    "O modelo achata a imagem no início e passa por uma camada oculta seguida de ReLU,\n",
    "e depois uma saída com 10 neurônios. Essa versão entrega logits brutos,\n",
    "então deve ser usada com a função CrossEntropyLoss (que aplica o softmax internamente).\n",
    "\n",
    "A escolha entre modelos mais simples ou mais profundos depende da tarefa, tempo de treino\n",
    "e capacidade computacional. Modelos mais profundos geralmente aprendem padrões mais complexos,\n",
    "mas podem levar mais tempo para treinar e exigir regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bf18a0f-1d31-4032-a762-8f49fdbf4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE 1 - Escolha um modelo antes de executar\n",
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28,128)\n",
    "        self.linear2 = nn.Linear(128,64)\n",
    "        self.linear3 = nn.Linear(64,10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return f.log_softmax(X,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39b938a3-1b9a-4d9e-b72b-e79d86096eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE 2 - Escolha um modelo antes de executar\n",
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.linear1 = nn.Linear(28 * 28, 128)   # Camada oculta com 128 neurônios\n",
    "        self.linear2 = nn.Linear(128, 10)        # Camada de saída com 10 neurônios (0 a 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)               # Achata a imagem de [1, 28, 28] para [784]\n",
    "        x = F.relu(self.linear1(x))              # Aplica ReLU na primeira camada\n",
    "        x = self.linear2(x)                      # Saída da rede (logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "657396cc-28c5-4bba-bdd5-56b23527b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo,trainloader,device):\n",
    "    otimizador = optim.SGD(modelo.parameters(),lr=0.01,momentum=0.5)\n",
    "    inicio = time()\n",
    "\n",
    "    criterio = nn.NLLLoss()\n",
    "    EPOCDS = 10\n",
    "    modelo.train()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0\n",
    "        for imagens, etiquetas in trainloader:\n",
    "            imagens = imagens.view(imagens.shape[0],-1)\n",
    "            otimizador.zero_grad()\n",
    "\n",
    "            output = modelo(imagens.to(device))\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device))\n",
    "\n",
    "            perda_instantanea.backward()\n",
    "\n",
    "            otimizador.step()\n",
    "\n",
    "            perda_acumulada += perda_instantanea.item()\n",
    "\n",
    "        else:\n",
    "            print(\"Epoch {} - Perda Resultante: {}\".format(epoch+1,perda_acumulada/len(trainloader)))\n",
    "    print(\"\\nTempo de treinamento (Min):\",(time()-inicio)/60)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "927b7094-3fde-430f-83e6-616bf9176848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo,valloader,device):\n",
    "    conta_corretas,conta_todas = 0,0\n",
    "    for imagens,etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1,784)\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device))\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            etiqueta_pred = probab.index(max(probab))\n",
    "            etiqueta_certa = etiquetas.numpy()[i]\n",
    "            if(etiqueta_certa == etiqueta_pred):\n",
    "                conta_corretas += 1\n",
    "            conta_todas += 1\n",
    "\n",
    "        print(\"Total de imagens testadas: \", conta_todas)\n",
    "        print(\"\\nPrecisão do modelo: {}%\",format(conta_corretas*100/conta_todas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdb27f65-4eea-4863-b888-5b1a7f0b1c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Modelo()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb1c3b-ec78-4158-9689-36ee1e378261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9a30f-8726-4da2-a8bd-8cc9f2f11278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
