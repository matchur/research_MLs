{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6113b8e-a623-4306-8f23-7b9e143c23cc",
   "metadata": {},
   "source": [
    "### Prepara√ß√£o do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d96fc9-e7e4-49d6-9e36-f6f72388a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: torch in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\matheus\\documents\\github projects\\research_mls\\ml_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy torch torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc992d-bf33-4e66-9294-680d9da55363",
   "metadata": {},
   "source": [
    "### Carregar e Explorar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668eaec0-ab4b-4ad9-9c46-5e73255a3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Post_ID   Platform     Hashtag Content_Type     Region    Views   Likes  \\\n",
      "0  Post_1     TikTok  #Challenge        Video         UK  4163464  339431   \n",
      "1  Post_2  Instagram  #Education       Shorts      India  4155940  215240   \n",
      "2  Post_3    Twitter  #Challenge        Video     Brazil  3666211  327143   \n",
      "3  Post_4    YouTube  #Education       Shorts  Australia   917951  127125   \n",
      "4  Post_5     TikTok      #Dance         Post     Brazil    64866  171361   \n",
      "\n",
      "   Shares  Comments Engagement_Level  \n",
      "0   53135     19346             High  \n",
      "1   65860     27239           Medium  \n",
      "2   39423     36223           Medium  \n",
      "3   11687     36806              Low  \n",
      "4   69581      6376           Medium  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Post_ID           5000 non-null   object\n",
      " 1   Platform          5000 non-null   object\n",
      " 2   Hashtag           5000 non-null   object\n",
      " 3   Content_Type      5000 non-null   object\n",
      " 4   Region            5000 non-null   object\n",
      " 5   Views             5000 non-null   int64 \n",
      " 6   Likes             5000 non-null   int64 \n",
      " 7   Shares            5000 non-null   int64 \n",
      " 8   Comments          5000 non-null   int64 \n",
      " 9   Engagement_Level  5000 non-null   object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 390.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Matheus\\\\Documents\\\\Github Projects\\\\research_MLs\\\\projects\\\\LMs\\\\dataset\\\\viral_social_media_trends\\\\Viral_Social_Media_Trends.csv\")\n",
    "\n",
    "# Exibir as primeiras linhas\n",
    "print(df.head())\n",
    "\n",
    "# Verificar informa√ß√µes gerais\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0a44c-33ef-4b4a-889c-f735386ba805",
   "metadata": {},
   "source": [
    "### Passo 3: Pr√©-processamento\n",
    "Converter vari√°veis categ√≥ricas (ex: Platform, Hashtag, Content_Type, Region) para valores num√©ricos usando LabelEncoder.\n",
    "\n",
    "Normalizar os valores num√©ricos (Views, Likes, Shares, Comments).\n",
    "\n",
    "Converter Engagement_Level em r√≥tulos num√©ricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877ac93d-3fa9-40b9-8605-1229ffdfa1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Post_ID  Platform  Hashtag  Content_Type  Region     Views     Likes  \\\n",
      "0  Post_1         1        0             5       6  0.832745  0.678653   \n",
      "1  Post_2         0        3             3       4  0.831240  0.429988   \n",
      "2  Post_3         2        0             5       1  0.733258  0.654049   \n",
      "3  Post_4         3        3             3       0  0.183404  0.253558   \n",
      "4  Post_5         1        2             1       1  0.012725  0.342131   \n",
      "\n",
      "     Shares  Comments  Engagement_Level  \n",
      "0  0.531223  0.386753                 0  \n",
      "1  0.658567  0.544692                 2  \n",
      "2  0.394002  0.724462                 2  \n",
      "3  0.116436  0.736128                 1  \n",
      "4  0.695805  0.127224                 2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "label_encoders = {}\n",
    "categorical_columns = [\"Platform\", \"Hashtag\", \"Content_Type\", \"Region\"]\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le  # Salvar para futuras transforma√ß√µes\n",
    "\n",
    "# Normalizar colunas num√©ricas\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = [\"Views\", \"Likes\", \"Shares\", \"Comments\"]\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Codificar a vari√°vel alvo\n",
    "engagement_encoder = LabelEncoder()\n",
    "df[\"Engagement_Level\"] = engagement_encoder.fit_transform(df[\"Engagement_Level\"])\n",
    "\n",
    "# Exibir o dataframe pr√©-processado\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca82417-a075-4294-988e-64a0324af2f0",
   "metadata": {},
   "source": [
    "### Passo 4: Dividir os Dados\n",
    "Dividimos os dados em treino (80%) e teste (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f0b59b-e8cf-4e96-b9d7-ddf44fe6bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Separar features (X) e r√≥tulos (y)\n",
    "X = df.drop(columns=[\"Post_ID\", \"Engagement_Level\"]).values\n",
    "y = df[\"Engagement_Level\"].values\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converter para tensores do PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c261d-9ffb-4fd8-bb2e-8af1dcad6177",
   "metadata": {},
   "source": [
    "### Passo 5: Criar o Modelo\n",
    "Agora, criamos uma rede neural simples para classificar o n√≠vel de engajamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c9d7fe-96af-45fe-ab43-2d8e9cb3572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EngagementModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EngagementModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 16)  # Camada oculta com 16 neur√¥nios\n",
    "        self.fc2 = nn.Linear(16, 8)  # Outra camada oculta\n",
    "        self.fc3 = nn.Linear(8, 3)   # Camada de sa√≠da para 3 classes (Baixo, M√©dio, Alto)\n",
    "        self.relu = nn.ReLU()  # Fun√ß√£o de ativa√ß√£o\n",
    "        self.softmax = nn.Softmax(dim=1)  # Ativa√ß√£o na sa√≠da\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Criar modelo\n",
    "model = EngagementModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f22bf-6dfe-4994-8f27-0f1e39b544c8",
   "metadata": {},
   "source": [
    "### Passo 6: Treinar o Modelo\n",
    "Definimos a fun√ß√£o de perda e o otimizador e treinamos o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe95195-fd2c-470d-9e36-bf4bf5158242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoca 10/50, Loss: 1.0978\n",
      "√âpoca 20/50, Loss: 1.0968\n",
      "√âpoca 30/50, Loss: 1.0957\n",
      "√âpoca 40/50, Loss: 1.0942\n",
      "√âpoca 50/50, Loss: 1.0925\n"
     ]
    }
   ],
   "source": [
    "# Definir fun√ß√£o de perda e otimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Treinamento\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"√âpoca {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90982b-f5b5-43b6-b1f1-71bdf1e71a91",
   "metadata": {},
   "source": [
    "### Passo 7: Avalia√ß√£o do Modelo\n",
    "Vamos medir acur√°cia, precis√£o, recall e F1-score no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d48b213-7aef-4281-b05d-0f094a12e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia: 0.37\n",
      "Precis√£o: 0.37\n",
      "Recall: 0.37\n",
      "F1-Score: 0.34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Modo de avalia√ß√£o\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test).argmax(dim=1)  # Pegamos a classe mais prov√°vel\n",
    "\n",
    "# Converter para numpy\n",
    "y_test_np = y_test.numpy()\n",
    "predictions_np = predictions.numpy()\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test_np, predictions_np)\n",
    "precision = precision_score(y_test_np, predictions_np, average=\"weighted\")\n",
    "recall = recall_score(y_test_np, predictions_np, average=\"weighted\")\n",
    "f1 = f1_score(y_test_np, predictions_np, average=\"weighted\")\n",
    "\n",
    "# Exibir resultados\n",
    "print(f\"Acur√°cia: {accuracy:.2f}\")\n",
    "print(f\"Precis√£o: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4556e64-725f-4375-8b30-cd04ef67a530",
   "metadata": {},
   "source": [
    "### Postagens de Teste\n",
    "Vamos definir algumas postagens com diferentes caracter√≠sticas e verificar o que o modelo prev√™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ab7f60-14ac-4db9-a5bb-688b1d6a31f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '#News'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Github Projects\\research_MLs\\ml_env\\Lib\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Github Projects\\research_MLs\\ml_env\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Github Projects\\research_MLs\\ml_env\\Lib\\site-packages\\sklearn\\utils\\_encode.py:167\u001b[39m, in \u001b[36m_nandict.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nan_value\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: '#News'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Aplicar o mesmo processamento do dataset original\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     df_test[col] = \u001b[43mlabel_encoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Converter para tensor PyTorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Github Projects\\research_MLs\\ml_env\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Github Projects\\research_MLs\\ml_env\\Lib\\site-packages\\sklearn\\utils\\_encode.py:237\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[31mValueError\u001b[39m: y contains previously unseen labels: '#News'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criar exemplos de postagens fict√≠cias\n",
    "test_posts = [\n",
    "    [\"TikTok\", \"#Viral\", \"Reel\", \"USA\", 1000000, 50000, 10000, 8000],  # Esperado: Alto\n",
    "    [\"Twitter\", \"#Dance\", \"Tweet\", \"Brazil\", 20000, 500, 100, 50],  # Esperado: Baixo\n",
    "    [\"Instagram\", \"#Education\", \"Post\", \"India\", 500000, 20000, 3000, 2500],  # Esperado: M√©dio\n",
    "]\n",
    "\n",
    "# Converter para DataFrame\n",
    "df_test = pd.DataFrame(test_posts, columns=[\"Platform\", \"Hashtag\", \"Content_Type\", \"Region\", \"Views\", \"Likes\", \"Shares\", \"Comments\"])\n",
    "\n",
    "# Aplicar o mesmo processamento do dataset original\n",
    "for col in categorical_columns:\n",
    "    df_test[col] = label_encoders[col].transform(df_test[col])\n",
    "\n",
    "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
    "\n",
    "# Converter para tensor PyTorch\n",
    "X_manual_test = torch.tensor(df_test.values, dtype=torch.float32)\n",
    "\n",
    "# Fazer previs√µes\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_manual_test).argmax(dim=1)\n",
    "\n",
    "# Decodificar r√≥tulos para legibilidade\n",
    "predicted_labels = engagement_encoder.inverse_transform(predictions.numpy())\n",
    "\n",
    "# Exibir os resultados\n",
    "for i, post in enumerate(test_posts):\n",
    "    print(f\"Postagem: {post}\")\n",
    "    print(f\"Previs√£o do Modelo: {predicted_labels[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae8d9c-72f0-4fd8-a3bb-6577eae08853",
   "metadata": {},
   "source": [
    "### An√°lise de Engajamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6abe30-332e-4385-ac5f-94798e948bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar o CSV\n",
    "df = pd.read_csv(\"social_media_data.csv\")\n",
    "\n",
    "# Plataforma com mais engajamento m√©dio\n",
    "platform_engagement = df.groupby(\"Platform\")[[\"Views\", \"Likes\", \"Shares\", \"Comments\"]].mean()\n",
    "platform_highest_engagement = platform_engagement.mean(axis=1).idxmax()\n",
    "\n",
    "# Hashtag mais curtida\n",
    "top_liked_hashtag = df.groupby(\"Hashtag\")[\"Likes\"].sum().idxmax()\n",
    "\n",
    "# Hashtag mais visualizada\n",
    "top_viewed_hashtag = df.groupby(\"Hashtag\")[\"Views\"].sum().idxmax()\n",
    "\n",
    "# Hashtag mais compartilhada\n",
    "top_shared_hashtag = df.groupby(\"Hashtag\")[\"Shares\"].sum().idxmax()\n",
    "\n",
    "# Pa√≠s que mais comenta\n",
    "top_commenting_country = df.groupby(\"Region\")[\"Comments\"].sum().idxmax()\n",
    "\n",
    "# Exibir resultados\n",
    "print(f\"üìå Plataforma com maior engajamento m√©dio: {platform_highest_engagement}\")\n",
    "print(f\"‚ù§Ô∏è Hashtag com mais curtidas: {top_liked_hashtag}\")\n",
    "print(f\"üëÄ Hashtag com mais visualiza√ß√µes: {top_viewed_hashtag}\")\n",
    "print(f\"üîÑ Hashtag mais compartilhada: {top_shared_hashtag}\")\n",
    "print(f\"üí¨ Pa√≠s que mais comenta: {top_commenting_country}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
